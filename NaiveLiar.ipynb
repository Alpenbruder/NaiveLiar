{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NaiveLiar\n",
    "During the lecute we discussed the problem that a neural network is forced to lie when faced with an unknown input. That makes the NN a navie liar. In real applications the training set may be filtered and only within the specified categories, but the real data isn't. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What I'm trying to do\n",
    "The easiest way to stop the NN from lying is to add another category. But that means changing the architecture of the network and retraining tha last layers. As that is ineffective, I don't wanna do that. Think about a scenario when we have a pretrained model, but no training dataset but are faced with the problem stated above.\n",
    "\n",
    "I will add a post-processing layer that adds another category that will be chosen when the NN is unsure. I also will create an extended test set that will have randomly generated pictures in it for checking the improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the engine warmed up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing Packages\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf \n",
    "print('Tensorflow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the MNIST dataset in one line\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train_normalized = x_train / 255 \n",
    "x_test_normalized = x_test /255\n",
    "\n",
    "# reshaping\n",
    "x_train= x_train_normalized.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test_normalized.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# defining the architecture\n",
    "merlin = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "merlin.summary()\n",
    "\n",
    "# Define your loss\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "merlin.compile(optimizer='adam',\n",
    "              loss = loss_fn ,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm setting up a pretty basic NN. I played around with some layers to improve the confidence and therefore the effectiveness of my post-processing. It is important that the output layer is a softmax because that will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training Merlin\n",
    "\n",
    "merlin.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs= 20,\n",
    "    batch_size= 512 ,\n",
    "    shuffle= True,\n",
    "    validation_data=(x_test, y_test),\n",
    "        callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),  \n",
    "        tf.keras.callbacks.ReduceLROnPlateau(factor=0.2, patience=3, min_lr=1e-6)  \n",
    "    ]\n",
    ")\n",
    "\n",
    "merlin.evaluate(x_test, y_test)\n",
    "\n",
    "# saving Merlin\n",
    "model_name = 'merlin.h5'\n",
    "merlin.save(model_name, save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I didn't have to train merlin for long. I added some callbacks to stop the training process early to prevent overfitting. I noticed that when I trained merlin for over 30 epochs it got more sure about the random pictures as well, like he has \"seen\" something in them. That made the overall accuracy worse again.\n",
    "\n",
    "I also saved merlin and did everything with the same model to research my approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model for further testing\n",
    "\n",
    "I have saved the model so everything should be reproducable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a saved merlin configuration you want to evaluate\n",
    "model_name = 'merlin.h5'\n",
    "merlin_reloaded = tf.keras.models.load_model(model_name)\n",
    "\n",
    "merlin_reloaded.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the extended dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random pictures for the \"miscellaneous\" category\n",
    "num_miscellaneous_samples = 1000  \n",
    "miscellaneous_images = np.random.rand(num_miscellaneous_samples, 28, 28, 1) \n",
    "miscellaneous_labels = np.full(num_miscellaneous_samples, 10)  # Labeling as 10\n",
    "\n",
    "# Concatenate the miscellaneous data with the original test data\n",
    "extended_x_test = np.concatenate([x_test, miscellaneous_images], axis=0)\n",
    "extended_y_test = np.concatenate([y_test, miscellaneous_labels], axis=0)\n",
    "\n",
    "# Shuffle test data\n",
    "extended_data = list(zip(extended_x_test, extended_y_test))\n",
    "np.random.shuffle(extended_data)\n",
    "shuffled_extended_x_test, shuffled_extended_y_test = zip(*extended_data)\n",
    "extended_x_test = np.array(shuffled_extended_x_test)\n",
    "extended_y_test = np.array(shuffled_extended_y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one core component of my reseaarch. I created an extended test data set by adding 1000 pictures where every pixel is random. This could theoretically generate every shape and form, like a number. But with such a small amount the chances are pretty low. I then shuffled the test data so the generated images are distributed over the dataset. So now about 9.1% of the dataset are random pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what happens when the AI is given something unknown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_extended = merlin_reloaded.predict(extended_x_test)\n",
    "\n",
    "for x in range(20):\n",
    "    print(\"Softmax values:\", predictions_extended[x], extended_y_test[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before adding anything, I wanted to know what the NN does when confronted with something it wasn't trained on. As expected, the confidence distribution is more even and it isn't that sure what it is anymore. This is also to check if my extened test set is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding custom post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "class PostProcessing(layers.Layer):\n",
    "    def __init__(self, threshold=0.2, **kwargs):\n",
    "        super(PostProcessing, self).__init__(**kwargs)\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def call(self, inputs):\n",
    "        custom_category = tf.where(tf.reduce_max(inputs, axis=-1) < self.threshold, 1.0, 0.0)\n",
    "        output = tf.concat([inputs, custom_category[..., tf.newaxis]], axis=-1)\n",
    "        return output\n",
    "\n",
    "# Adding the layer to merlin\n",
    "\n",
    "merlin_with_postprocessing= models.Sequential([\n",
    "    merlin_reloaded,\n",
    "    PostProcessing(threshold=0.94)\n",
    "])\n",
    "\n",
    "\n",
    "merlin_with_postprocessing.compile(optimizer='adam',\n",
    "              loss = loss_fn ,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I add a post-processing layer to the already trained merlin. The call function creates a new category with the dimensions of the input vector and sets it to 1 if anything is below the threshold or to 0 if not. As the highest probability is chosen and the other values are always lower than 1, the custom category is chosen as true. I know this breaks the rule of the sum of probabilities being 1, but it also preserves the original values for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the raw output to see if everything is alright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_extended = merlin_with_postprocessing.predict(extended_x_test)\n",
    "\n",
    "for x in range(20):\n",
    "    print(\"Values:\", predictions_extended[x], extended_y_test[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the accuracy when adding custom post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare standard to extended set\n",
    "print(\"Standard set:\")\n",
    "print('shape', y_test.shape)\n",
    "merlin_reloaded.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"Extended set with postprocessing, trying different thresholds:\")\n",
    "print('shape', extended_y_test.shape)\n",
    "\n",
    "for x in range(100):\n",
    "    merlin_with_postprocessing_test= models.Sequential([\n",
    "    merlin_reloaded,\n",
    "    AddCustomCategoryLayer(threshold=x/100)])\n",
    "\n",
    "    merlin_with_postprocessing_test.compile(optimizer='adam',\n",
    "              loss = loss_fn ,\n",
    "              metrics=['accuracy'])\n",
    "    print(x, \"%\")\n",
    "    merlin_with_postprocessing_test.evaluate(extended_x_test, extended_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the base NN worls on the original data set can be seen when setting the threshold to 0%, basically skipping the post-processing. With about 9% of the dataset being random we can see that the accuraacy also drops about 9% without post-processing to 89.87%.\n",
    "\n",
    "By running this code the best threshold can be determined. With it set to 94% an accuracy of 95.75% can be achieved, which is 3.11% lower than the baseline. But comparing this to the performance of the baseline model on the altered data set, the added post-processing improves the accuracy by 5.88%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving into the results\n",
    "I just changed the code a bit so it woks with the added category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = merlin_with_postprocessing.predict(extended_x_test)\n",
    "\n",
    "# We want him to assign the digit class with the highest probability to the sample.\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "pd.DataFrame(predictions)\n",
    "\n",
    "# Plot for the intuitive approach\n",
    "\n",
    "numbers_to_display = 196\n",
    "num_cells = math.ceil(math.sqrt(numbers_to_display))\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for plot_index in range(numbers_to_display):    \n",
    "    predicted_label = predictions[plot_index]\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    color_map = 'Greens' if predicted_label == extended_y_test[plot_index] else 'Reds'\n",
    "    plt.subplot(num_cells, num_cells, plot_index + 1)\n",
    "    plt.imshow(extended_x_test[plot_index].reshape((28, 28)), cmap=color_map)\n",
    "    plt.xlabel(predicted_label)\n",
    "\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = tf.math.confusion_matrix(extended_y_test, predictions)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(9, 7))\n",
    "sn.heatmap(\n",
    "    confusion_matrix,\n",
    "    annot=True,\n",
    "    linewidths=.7,\n",
    "    fmt=\"d\",\n",
    "    square=True,\n",
    "    ax=ax,\n",
    "    cmap=\"viridis\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As it can be seen in the improvement of the accuracy the post-processing works. But there are definitely better approaches to this problem that can determine if something is detectable. But as I am a programmer that still thinks in C++, this method seemed appealing to me. \n",
    "\n",
    "The confusion matrix shows that a lot of fale positives, but little to no false negatives. This suggests lowering the threshold. I have looked into this, but the false positives are not getting lower as fast as the false negatives go up.\n",
    "\n",
    "The AI approach would be to train a NN to determine if my network is confused or not. The oputput of the confusion NN would either be a binaray indicator whether it's confused or all 11 cantegories. This would be very resource-heavy beut probably fun to try out some day."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
